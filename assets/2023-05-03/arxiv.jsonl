{"title":"Generalizing Dataset Distillation via Deep Generative Prior","description":"Dataset Distillation aims to distill an entire dataset's knowledge into a few synthetic images. The idea is to synthesize a small number of synthetic data points that, when given to a learning algorithm as training data, result in a model approximating one trained on the original data. Despite recent progress in the field, existing dataset distillation methods fail to generalize to new architectures and scale to high-resolution datasets. To overcome the above issues, we propose to use the learned prior from pre-trained deep generative models to synthesize the distilled data. To achieve this, we present a new optimization algorithm that distills a large number of images into a few intermediate feature vectors in the generative model's latent space. Our method augments existing techniques, significantly improving cross-architecture generalization in all settings.","link":"http://arxiv.org/abs/2305.01649v1","created":"2023-05-02","tags":["arxiv","dataset"],"meta":{"query":"ti:dataset OR ti:corpus OR ti:database OR abs:\"a new dataset\""}}
{"title":"ContactArt: Learning 3D Interaction Priors for Category-level Articulated Object and Hand Poses Estimation","description":"We propose a new dataset and a novel approach to learning hand-object interaction priors for hand and articulated object pose estimation. We first collect a dataset using visual teleoperation, where the human operator can directly play within a physical simulator to manipulate the articulated objects. We record the data and obtain free and accurate annotations on object poses and contact information from the simulator. Our system only requires an iPhone to record human hand motion, which can be easily scaled up and largely lower the costs of data and annotation collection. With this data, we learn 3D interaction priors including a discriminator (in a GAN) capturing the distribution of how object parts are arranged, and a diffusion model which generates the contact regions on articulated objects, guiding the hand pose estimation. Such structural and contact priors can easily transfer to real-world data with barely any domain gap. By using our data and learned priors, our method significantly improves the performance on joint hand and articulated object poses estimation over the existing state-of-the-art methods. The project is available at https://zehaozhu.github.io/ContactArt/ .","link":"http://arxiv.org/abs/2305.01618v1","created":"2023-05-02","tags":["arxiv","dataset"],"meta":{"query":"ti:dataset OR ti:corpus OR ti:database OR abs:\"a new dataset\""}}
{"title":"Analysis of Dispersive Fourier Transform dataset using Dynamic Mode Decomposition: evidence of multiple vibrational modes, and their interplay in a three-soliton molecule","description":"We demonstrate that the Dynamic Mode Decomposition technique can effectively reduce the amount of noise in Dispersive Fourier Transform dataset; and allow for finer quantitative analysis of the experimental data. We therefore were able to demonstrate that the oscillation pattern of a soliton molecule actually results from the interplay of several elementary vibration modes.","link":"http://arxiv.org/abs/2305.01591v1","created":"2023-05-02","tags":["arxiv","dataset"],"meta":{"query":"ti:dataset OR ti:corpus OR ti:database OR abs:\"a new dataset\""}}
{"title":"Pick-a-Pic: An Open Dataset of User Preferences for Text-to-Image Generation","description":"The ability to collect a large dataset of human preferences from text-to-image users is usually limited to companies, making such datasets inaccessible to the public. To address this issue, we create a web app that enables text-to-image users to generate images and specify their preferences. Using this web app we build Pick-a-Pic, a large, open dataset of text-to-image prompts and real users' preferences over generated images. We leverage this dataset to train a CLIP-based scoring function, PickScore, which exhibits superhuman performance on the task of predicting human preferences. Then, we test PickScore's ability to perform model evaluation and observe that it correlates better with human rankings than other automatic evaluation metrics. Therefore, we recommend using PickScore for evaluating future text-to-image generation models, and using Pick-a-Pic prompts as a more relevant dataset than MS-COCO. Finally, we demonstrate how PickScore can enhance existing text-to-image models via ranking.","link":"http://arxiv.org/abs/2305.01569v1","created":"2023-05-02","tags":["arxiv","dataset"],"meta":{"query":"ti:dataset OR ti:corpus OR ti:database OR abs:\"a new dataset\""}}
{"title":"FIREBALL: A Dataset of Dungeons and Dragons Actual-Play with Structured Game State Information","description":"Dungeons & Dragons (D&D) is a tabletop roleplaying game with complex natural language interactions between players and hidden state information. Recent work has shown that large language models (LLMs) that have access to state information can generate higher quality game turns than LLMs that use dialog history alone. However, previous work used game state information that was heuristically created and was not a true gold standard game state. We present FIREBALL, a large dataset containing nearly 25,000 unique sessions from real D\\&D gameplay on Discord with true game state info. We recorded game play sessions of players who used the Avrae bot, which was developed to aid people in playing D&D online, capturing language, game commands and underlying game state information. We demonstrate that FIREBALL can improve natural language generation (NLG) by using Avrae state information, improving both automated metrics and human judgments of quality. Additionally, we show that LLMs can generate executable Avrae commands, particularly after finetuning.","link":"http://arxiv.org/abs/2305.01528v1","created":"2023-05-02","tags":["arxiv","dataset"],"meta":{"query":"ti:dataset OR ti:corpus OR ti:database OR abs:\"a new dataset\""}}
{"title":"Huatuo-26M, a Large-scale Chinese Medical QA Dataset","description":"In this paper, we release a largest ever medical Question Answering (QA) dataset with 26 million QA pairs. We benchmark many existing approaches in our dataset in terms of both retrieval and generation. Experimental results show that the existing models perform far lower than expected and the released dataset is still challenging in the pre-trained language model era. Moreover, we also experimentally show the benefit of the proposed dataset in many aspects: (i) trained models for other QA datasets in a zero-shot fashion; and (ii) as external knowledge for retrieval-augmented generation (RAG); and (iii) improving existing pre-trained language models by using the QA pairs as a pre-training corpus in continued training manner. We believe that this dataset will not only contribute to medical research but also facilitate both the patients and clinical doctors. See \\url{https://github.com/FreedomIntelligence/Huatuo-26M}.","link":"http://arxiv.org/abs/2305.01526v1","created":"2023-05-02","tags":["arxiv","dataset"],"meta":{"query":"ti:dataset OR ti:corpus OR ti:database OR abs:\"a new dataset\""}}
{"title":"Multimodal Neural Databases","description":"The rise in loosely-structured data available through text, images, and other modalities has called for new ways of querying them. Multimedia Information Retrieval has filled this gap and has witnessed exciting progress in recent years. Tasks such as search and retrieval of extensive multimedia archives have undergone massive performance improvements, driven to a large extent by recent developments in multimodal deep learning. However, methods in this field remain limited in the kinds of queries they support and, in particular, their inability to answer database-like queries. For this reason, inspired by recent work on neural databases, we propose a new framework, which we name Multimodal Neural Databases (MMNDBs). MMNDBs can answer complex database-like queries that involve reasoning over different input modalities, such as text and images, at scale. In this paper, we present the first architecture able to fulfill this set of requirements and test it with several baselines, showing the limitations of currently available models. The results show the potential of these new techniques to process unstructured data coming from different modalities, paving the way for future research in the area. Code to replicate the experiments will be released at https://github.com/GiovanniTRA/MultimodalNeuralDatabases","link":"http://arxiv.org/abs/2305.01447v1","created":"2023-05-02","tags":["arxiv","dataset"],"meta":{"query":"ti:dataset OR ti:corpus OR ti:database OR abs:\"a new dataset\""}}
{"title":"Towards a better labeling process for network security datasets","description":"Most network security datasets do not have comprehensive label assignment criteria, hindering the evaluation of the datasets, the training of models, the results obtained, the comparison with other methods, and the evaluation in real-life scenarios. There is no labeling ontology nor tools to help assign the labels, resulting in most analyzed datasets assigning labels in files or directory names. This paper addresses the problem of having a better labeling process by (i) reviewing the needs of stakeholders of the datasets, from creators to model users, (ii) presenting a new ontology of label assignment, (iii) presenting a new tool for assigning structured labels for Zeek network flows based on the ontology, and (iv) studying the differences between generating labels and consuming labels in real-life scenarios. We conclude that a process for structured label assignment is paramount for advancing research in network security and that the new ontology-based label assignation rules should be published as an artifact of every dataset.","link":"http://arxiv.org/abs/2305.01337v1","created":"2023-05-02","tags":["arxiv","dataset"],"meta":{"query":"ti:dataset OR ti:corpus OR ti:database OR abs:\"a new dataset\""}}
{"title":"Towards a Critical Open-Source Software Database","description":"Open-source software (OSS) plays a vital role in the modern software ecosystem. However, the maintenance and sustainability of OSS projects can be challenging. In this paper, we present the CrOSSD project, which aims to build a database of OSS projects and measure their current project \"health\" status. In the project, we will use both quantitative and qualitative metrics to evaluate the health of OSS projects. The quantitative metrics will be gathered through automated crawling of meta information such as the number of contributors, commits and lines of code. Qualitative metrics will be gathered for selected \"critical\" projects through manual analysis and automated tools, including aspects such as sustainability, funding, community engagement and adherence to security policies. The results of the analysis will be presented on a user-friendly web platform, which will allow users to view the health of individual OSS projects as well as the overall health of the OSS ecosystem. With this approach, the CrOSSD project provides a comprehensive and up-to-date view of the health of OSS projects, making it easier for developers, maintainers and other stakeholders to understand the health of OSS projects and make informed decisions about their use and maintenance.","link":"http://arxiv.org/abs/2305.01311v1","created":"2023-05-02","tags":["arxiv","dataset"],"meta":{"query":"ti:dataset OR ti:corpus OR ti:database OR abs:\"a new dataset\""}}
{"title":"HTPS: Heterogeneous Transferring Prediction System for Healthcare Datasets","description":"Medical internet of things leads to revolutionary improvements in medical services, also known as smart healthcare. With the big healthcare data, data mining and machine learning can assist wellness management and intelligent diagnosis, and achieve the P4-medicine. However, healthcare data has high sparsity and heterogeneity. In this paper, we propose a Heterogeneous Transferring Prediction System (HTPS). Feature engineering mechanism transforms the dataset into sparse and dense feature matrices, and autoencoders in the embedding networks not only embed features but also transfer knowledge from heterogeneous datasets. Experimental results show that the proposed HTPS outperforms the benchmark systems on various prediction tasks and datasets, and ablation studies present the effectiveness of each designed mechanism. Experimental results demonstrate the negative impact of heterogeneous data on benchmark systems and the high transferability of the proposed HTPS.","link":"http://arxiv.org/abs/2305.01252v1","created":"2023-05-02","tags":["arxiv","dataset"],"meta":{"query":"ti:dataset OR ti:corpus OR ti:database OR abs:\"a new dataset\""}}
{"title":"MultiLegalSBD: A Multilingual Legal Sentence Boundary Detection Dataset","description":"Sentence Boundary Detection (SBD) is one of the foundational building blocks of Natural Language Processing (NLP), with incorrectly split sentences heavily influencing the output quality of downstream tasks. It is a challenging task for algorithms, especially in the legal domain, considering the complex and different sentence structures used. In this work, we curated a diverse multilingual legal dataset consisting of over 130'000 annotated sentences in 6 languages. Our experimental results indicate that the performance of existing SBD models is subpar on multilingual legal data. We trained and tested monolingual and multilingual models based on CRF, BiLSTM-CRF, and transformers, demonstrating state-of-the-art performance. We also show that our multilingual models outperform all baselines in the zero-shot setting on a Portuguese test set. To encourage further research and development by the community, we have made our dataset, models, and code publicly available.","link":"http://arxiv.org/abs/2305.01211v1","created":"2023-05-02","tags":["arxiv","dataset"],"meta":{"query":"ti:dataset OR ti:corpus OR ti:database OR abs:\"a new dataset\""}}
{"title":"Topic Shift Detection in Chinese Dialogues: Corpus and Benchmark","description":"Dialogue topic shift detection is to detect whether an ongoing topic has shifted or should shift in a dialogue, which can be divided into two categories, i.e., response-known task and response-unknown task. Currently, only a few investigated the latter, because it is still a challenge to predict the topic shift without the response information. In this paper, we first annotate a Chinese Natural Topic Dialogue (CNTD) corpus consisting of 1308 dialogues to fill the gap in the Chinese natural conversation topic corpus. And then we focus on the response-unknown task and propose a teacher-student framework based on hierarchical contrastive learning to predict the topic shift without the response. Specifically, the response at high-level teacher-student is introduced to build the contrastive learning between the response and the context, while the label contrastive learning is constructed at low-level student. The experimental results on our Chinese CNTD and English TIAGE show the effectiveness of our proposed model.","link":"http://arxiv.org/abs/2305.01195v1","created":"2023-05-02","tags":["arxiv","dataset"],"meta":{"query":"ti:dataset OR ti:corpus OR ti:database OR abs:\"a new dataset\""}}
{"title":"Differentially Private In-Context Learning","description":"An important question in deploying large language models (LLMs) is how to augment LLMs with private data. We propose Differentially Private In-context Learning (DP-ICL) to enable LLMs to adapt to new tasks while maintaining privacy guarantees. DP-ICL performs private inference by establishing noisy consensus over an ensemble of exemplars using the Report-Noisy-Max mechanism. We evaluate DP-ICL on four benchmarks and find that it achieves comparable performance (<2\\% degradation) with non-private ICL.","link":"http://arxiv.org/abs/2305.01639v1","created":"2023-05-02","tags":["arxiv","ml","security"],"meta":{"query":"machine AND learning AND security OR machine AND learning AND secure"}}
{"title":"Mitigating Approximate Memorization in Language Models via Dissimilarity Learned Policy","description":"Large Language models (LLMs) are trained on large amounts of data, which can include sensitive information that may compromise personal privacy. LLMs showed to memorize parts of the training data and emit those data verbatim when an adversary prompts appropriately. Previous research has primarily focused on data preprocessing and differential privacy techniques to address memorization or prevent verbatim memorization exclusively, which can give a false sense of privacy. However, these methods rely on explicit and implicit assumptions about the structure of the data to be protected, which often results in an incomplete solution to the problem. To address this, we propose a novel framework that utilizes a reinforcement learning approach (PPO) to fine-tune LLMs to mitigate approximate memorization. Our approach utilizes a negative similarity score, such as BERTScore or SacreBLEU, as a reward signal to learn a dissimilarity policy. Our results demonstrate that this framework effectively mitigates approximate memorization while maintaining high levels of coherence and fluency in the generated samples. Furthermore, our framework is robust in mitigating approximate memorization across various circumstances, including longer context, which is known to increase memorization in LLMs.","link":"http://arxiv.org/abs/2305.01550v1","created":"2023-05-02","tags":["arxiv","ml","security"],"meta":{"query":"machine AND learning AND security OR machine AND learning AND secure"}}
{"title":"Efficient Federated Learning with Enhanced Privacy via Lottery Ticket Pruning in Edge Computing","description":"Federated learning (FL) is a collaborative learning paradigm for decentralized private data from mobile terminals (MTs). However, it suffers from issues in terms of communication, resource of MTs, and privacy. Existing privacy-preserving FL methods usually adopt the instance-level differential privacy (DP), which provides a rigorous privacy guarantee but with several bottlenecks: severe performance degradation, transmission overhead, and resource constraints of edge devices such as MTs. To overcome these drawbacks, we propose Fed-LTP, an efficient and privacy-enhanced FL framework with \\underline{\\textbf{L}}ottery \\underline{\\textbf{T}}icket \\underline{\\textbf{H}}ypothesis (LTH) and zero-concentrated D\\underline{\\textbf{P}} (zCDP). It generates a pruned global model on the server side and conducts sparse-to-sparse training from scratch with zCDP on the client side. On the server side, two pruning schemes are proposed: (i) the weight-based pruning (LTH) determines the pruned global model structure; (ii) the iterative pruning further shrinks the size of the pruned model's parameters. Meanwhile, the performance of Fed-LTP is also boosted via model validation based on the Laplace mechanism. On the client side, we use sparse-to-sparse training to solve the resource-constraints issue and provide tighter privacy analysis to reduce the privacy budget. We evaluate the effectiveness of Fed-LTP on several real-world datasets in both independent and identically distributed (IID) and non-IID settings. The results clearly confirm the superiority of Fed-LTP over state-of-the-art (SOTA) methods in communication, computation, and memory efficiencies while realizing a better utility-privacy trade-off.","link":"http://arxiv.org/abs/2305.01387v1","created":"2023-05-02","tags":["arxiv","ml","security"],"meta":{"query":"machine AND learning AND security OR machine AND learning AND secure"}}
{"title":"DABS: Data-Agnostic Backdoor attack at the Server in Federated Learning","description":"Federated learning (FL) attempts to train a global model by aggregating local models from distributed devices under the coordination of a central server. However, the existence of a large number of heterogeneous devices makes FL vulnerable to various attacks, especially the stealthy backdoor attack. Backdoor attack aims to trick a neural network to misclassify data to a target label by injecting specific triggers while keeping correct predictions on original training data. Existing works focus on client-side attacks which try to poison the global model by modifying the local datasets. In this work, we propose a new attack model for FL, namely Data-Agnostic Backdoor attack at the Server (DABS), where the server directly modifies the global model to backdoor an FL system. Extensive simulation results show that this attack scheme achieves a higher attack success rate compared with baseline methods while maintaining normal accuracy on the clean data.","link":"http://arxiv.org/abs/2305.01267v1","created":"2023-05-02","tags":["arxiv","ml","security"],"meta":{"query":"machine AND learning AND security OR machine AND learning AND secure"}}
{"title":"MDENet: Multi-modal Dual-embedding Networks for Malware Open-set Recognition","description":"Malware open-set recognition (MOSR) aims at jointly classifying malware samples from known families and detect the ones from novel unknown families, respectively. Existing works mostly rely on a well-trained classifier considering the predicted probabilities of each known family with a threshold-based detection to achieve the MOSR. However, our observation reveals that the feature distributions of malware samples are extremely similar to each other even between known and unknown families. Thus the obtained classifier may produce overly high probabilities of testing unknown samples toward known families and degrade the model performance. In this paper, we propose the Multi-modal Dual-Embedding Networks, dubbed MDENet, to take advantage of comprehensive malware features (i.e., malware images and malware sentences) from different modalities to enhance the diversity of malware feature space, which is more representative and discriminative for down-stream recognition. Last, to further guarantee the open-set recognition, we dually embed the fused multi-modal representation into one primary space and an associated sub-space, i.e., discriminative and exclusive spaces, with contrastive sampling and rho-bounded enclosing sphere regularizations, which resort to classification and detection, respectively. Moreover, we also enrich our previously proposed large-scaled malware dataset MAL-100 with multi-modal characteristics and contribute an improved version dubbed MAL-100+. Experimental results on the widely used malware dataset Mailing and the proposed MAL-100+ demonstrate the effectiveness of our method.","link":"http://arxiv.org/abs/2305.01245v1","created":"2023-05-02","tags":["arxiv","ml","security"],"meta":{"query":"machine AND learning AND security OR machine AND learning AND secure"}}
{"title":"CNS-Net: Conservative Novelty Synthesizing Network for Malware Recognition in an Open-set Scenario","description":"We study the challenging task of malware recognition on both known and novel unknown malware families, called malware open-set recognition (MOSR). Previous works usually assume the malware families are known to the classifier in a close-set scenario, i.e., testing families are the subset or at most identical to training families. However, novel unknown malware families frequently emerge in real-world applications, and as such, require to recognize malware instances in an open-set scenario, i.e., some unknown families are also included in the test-set, which has been rarely and non-thoroughly investigated in the cyber-security domain. One practical solution for MOSR may consider jointly classifying known and detecting unknown malware families by a single classifier (e.g., neural network) from the variance of the predicted probability distribution on known families. However, conventional well-trained classifiers usually tend to obtain overly high recognition probabilities in the outputs, especially when the instance feature distributions are similar to each other, e.g., unknown v.s. known malware families, and thus dramatically degrades the recognition on novel unknown malware families. In this paper, we propose a novel model that can conservatively synthesize malware instances to mimic unknown malware families and support a more robust training of the classifier. Moreover, we also build a new large-scale malware dataset, named MAL-100, to fill the gap of lacking large open-set malware benchmark dataset. Experimental results on two widely used malware datasets and our MAL-100 demonstrate the effectiveness of our model compared with other representative methods.","link":"http://arxiv.org/abs/2305.01236v1","created":"2023-05-02","tags":["arxiv","ml","security"],"meta":{"query":"machine AND learning AND security OR machine AND learning AND secure"}}
{"title":"Stratified Adversarial Robustness with Rejection","description":"Recently, there is an emerging interest in adversarially training a classifier with a rejection option (also known as a selective classifier) for boosting adversarial robustness. While rejection can incur a cost in many applications, existing studies typically associate zero cost with rejecting perturbed inputs, which can result in the rejection of numerous slightly-perturbed inputs that could be correctly classified. In this work, we study adversarially-robust classification with rejection in the stratified rejection setting, where the rejection cost is modeled by rejection loss functions monotonically non-increasing in the perturbation magnitude. We theoretically analyze the stratified rejection setting and propose a novel defense method -- Adversarial Training with Consistent Prediction-based Rejection (CPR) -- for building a robust selective classifier. Experiments on image datasets demonstrate that the proposed method significantly outperforms existing methods under strong adaptive attacks. For instance, on CIFAR-10, CPR reduces the total robust loss (for different rejection losses) by at least 7.3% under both seen and unseen attacks.","link":"http://arxiv.org/abs/2305.01139v1","created":"2023-05-02","tags":["arxiv","ml","security"],"meta":{"query":"machine AND learning AND security OR machine AND learning AND secure"}}
{"title":"New Trends in Machine Translation using Large Language Models: Case Examples with ChatGPT","description":"Machine Translation (MT) has made significant progress in recent years using deep learning, especially after the emergence of large language models (LLMs) such as GPT-3 and ChatGPT. This brings new challenges and opportunities for MT using LLMs. In this paper, we brainstorm some interesting directions for MT using LLMs, including stylized MT, interactive MT, and Translation Memory-based MT, as well as a new evaluation paradigm using LLMs. We also discuss the privacy concerns in MT using LLMs and a basic privacy-preserving method to mitigate such risks. To illustrate the potential of our proposed directions, we present several examples for the new directions mentioned above, demonstrating the feasibility of the proposed directions and highlight the opportunities and challenges for future research in MT using LLMs.","link":"http://arxiv.org/abs/2305.01181v1","created":"2023-05-02","tags":["arxiv","ml","prompteng"],"meta":{"query":"abs:\"chatgpt\""}}
{"title":"On the Impact of Data Quality on Image Classification Fairness","description":"With the proliferation of algorithmic decision-making, increased scrutiny has been placed on these systems. This paper explores the relationship between the quality of the training data and the overall fairness of the models trained with such data in the context of supervised classification. We measure key fairness metrics across a range of algorithms over multiple image classification datasets that have a varying level of noise in both the labels and the training data itself. We describe noise in the labels as inaccuracies in the labelling of the data in the training set and noise in the data as distortions in the data, also in the training set. By adding noise to the original datasets, we can explore the relationship between the quality of the training data and the fairness of the output of the models trained on that data.","link":"http://arxiv.org/abs/2305.01595v1","created":"2023-05-02","tags":["arxiv","data-quality"],"meta":{"query":"data AND quality"}}
{"title":"Turning Flowchart into Dialog: Plan-based Data Augmentation for Low-Resource Flowchart-grounded Troubleshooting Dialogs","description":"Flowchart-grounded troubleshooting dialogue (FTD) systems, which follow the instructions of a flowchart to diagnose users' problems in specific domains (eg., vehicle, laptop), have been gaining research interest in recent years. However, collecting sufficient dialogues that are naturally grounded on flowcharts is costly, thus FTD systems are impeded by scarce training data. To mitigate the data sparsity issue, we propose a plan-based data augmentation (PlanDA) approach that generates diverse synthetic dialog data at scale by transforming concise flowchart into dialogues. Specifically, its generative model employs a variational-base framework with a hierarchical planning strategy that includes global and local latent planning variables. Experiments on the FloDial dataset show that synthetic dialogue produced by PlanDA improves the performance of downstream tasks, including flowchart path retrieval and response generation, in particular on the Out-of-Flowchart settings. In addition, further analysis demonstrate the quality of synthetic data generated by PlanDA in paths that are covered by current sample dialogues and paths that are not covered.","link":"http://arxiv.org/abs/2305.01323v1","created":"2023-05-02","tags":["arxiv","data-quality"],"meta":{"query":"data AND quality"}}
{"title":"Geometric Prior Based Deep Human Point Cloud Geometry Compression","description":"The emergence of digital avatars has raised an exponential increase in the demand for human point clouds with realistic and intricate details. The compression of such data becomes challenging with overwhelming data amounts comprising millions of points. Herein, we leverage the human geometric prior in geometry redundancy removal of point clouds, greatly promoting the compression performance. More specifically, the prior provides topological constraints as geometry initialization, allowing adaptive adjustments with a compact parameter set that could be represented with only a few bits. Therefore, we can envisage high-resolution human point clouds as a combination of geometric priors and structural deviations. The priors could first be derived with an aligned point cloud, and subsequently the difference of features is compressed into a compact latent code. The proposed framework can operate in a play-and-plug fashion with existing learning based point cloud compression methods. Extensive experimental results show that our approach significantly improves the compression performance without deteriorating the quality, demonstrating its promise in a variety of applications.","link":"http://arxiv.org/abs/2305.01309v1","created":"2023-05-02","tags":["arxiv","data-quality"],"meta":{"query":"data AND quality"}}
{"title":"TangleSim: An Agent-based, Modular Simulator for DAG-based Distributed Ledger Technologies","description":"DAG-based DLTs allow for parallel, asynchronous writing access to a ledger. Consequently, the perception of the most recent blocks may differ considerably between nodes, and the underlying network properties of the P2P layer have a direct impact on the performance of the protocol. Moreover, the stronger inter-dependencies of several core components demand a more complex and complete approach to studying such DLTs. This paper presents an agent-based, open-sourced simulator for large-scale networks that implement the leaderless Tangle 2.0 consensus protocol. Its scope includes modelling the underlying peer-to-peer communication with network topology, package loss, heterogeneous latency, the gossip protocol with reliable broadcast qualities, the underlying DAG-based data structure, and the consensus protocol. The simulator allows us to explore the performance of the protocol in different network environments, as well as different attack scenarios.","link":"http://arxiv.org/abs/2305.01232v1","created":"2023-05-02","tags":["arxiv","data-quality"],"meta":{"query":"data AND quality"}}
{"title":"MultiLegalSBD: A Multilingual Legal Sentence Boundary Detection Dataset","description":"Sentence Boundary Detection (SBD) is one of the foundational building blocks of Natural Language Processing (NLP), with incorrectly split sentences heavily influencing the output quality of downstream tasks. It is a challenging task for algorithms, especially in the legal domain, considering the complex and different sentence structures used. In this work, we curated a diverse multilingual legal dataset consisting of over 130'000 annotated sentences in 6 languages. Our experimental results indicate that the performance of existing SBD models is subpar on multilingual legal data. We trained and tested monolingual and multilingual models based on CRF, BiLSTM-CRF, and transformers, demonstrating state-of-the-art performance. We also show that our multilingual models outperform all baselines in the zero-shot setting on a Portuguese test set. To encourage further research and development by the community, we have made our dataset, models, and code publicly available.","link":"http://arxiv.org/abs/2305.01211v1","created":"2023-05-02","tags":["arxiv","data-quality"],"meta":{"query":"data AND quality"}}
{"title":"The AI Revolution in Education: Will AI Replace or Assist Teachers in Higher Education?","description":"This paper explores the potential of artificial intelligence (AI) in higher education, specifically its capacity to replace or assist human teachers. By reviewing relevant literature and analysing survey data from students and teachers, the study provides a comprehensive perspective on the future role of educators in the face of advancing AI technologies. Findings suggest that although some believe AI may eventually replace teachers, the majority of participants argue that human teachers possess unique qualities, such as critical thinking, creativity, and emotions, which make them irreplaceable. The study also emphasizes the importance of social-emotional competencies developed through human interactions, which AI technologies cannot currently replicate. The research proposes that teachers can effectively integrate AI to enhance teaching and learning without viewing it as a replacement. To do so, teachers need to understand how AI can work well with teachers and students while avoiding potential pitfalls, develop AI literacy, and address practical issues such as data protection, ethics, and privacy. The study reveals that students value and respect human teachers, even as AI becomes more prevalent in education. The study also introduces a roadmap for students, teachers, and universities. This roadmap serves as a valuable guide for refining teaching skills, fostering personal connections, and designing curriculums that effectively balance the strengths of human educators with AI technologies. The future of education lies in the synergy between human teachers and AI. By understanding and refining their unique qualities, teachers, students, and universities can effectively navigate the integration of AI, ensuring a well-rounded and impactful learning experience.","link":"http://arxiv.org/abs/2305.01185v1","created":"2023-05-02","tags":["arxiv","data-quality"],"meta":{"query":"data AND quality"}}
{"title":"Faster OreFSDet : A Lightweight and Effective Few-shot Object Detector for Ore Images","description":"For the ore particle size detection, obtaining a sizable amount of high-quality ore labeled data is time-consuming and expensive. General object detection methods often suffer from severe over-fitting with scarce labeled data. Despite their ability to eliminate over-fitting, existing few-shot object detectors encounter drawbacks such as slow detection speed and high memory requirements, making them difficult to implement in a real-world deployment scenario. To this end, we propose a lightweight and effective few-shot detector to achieve competitive performance with general object detection with only a few samples for ore images. First, the proposed support feature mining block characterizes the importance of location information in support features. Next, the relationship guidance block makes full use of support features to guide the generation of accurate candidate proposals. Finally, the dual-scale semantic aggregation module retrieves detailed features at different resolutions to contribute with the prediction process. Experimental results show that our method consistently exceeds the few-shot detectors with an excellent performance gap on all metrics. Moreover, our method achieves the smallest model size of 19MB as well as being competitive at 50 FPS detection speed compared with general object detectors. The source code is available at https://github.com/MVME-HBUT/Faster-OreFSDet.","link":"http://arxiv.org/abs/2305.01183v1","created":"2023-05-02","tags":["arxiv","data-quality"],"meta":{"query":"data AND quality"}}
{"title":"High-Fidelity Image Synthesis from Pulmonary Nodule Lesion Maps using Semantic Diffusion Model","description":"Lung cancer has been one of the leading causes of cancer-related deaths worldwide for years. With the emergence of deep learning, computer-assisted diagnosis (CAD) models based on learning algorithms can accelerate the nodule screening process, providing valuable assistance to radiologists in their daily clinical workflows. However, developing such robust and accurate models often requires large-scale and diverse medical datasets with high-quality annotations. Generating synthetic data provides a pathway for augmenting datasets at a larger scale. Therefore, in this paper, we explore the use of Semantic Diffusion Mod- els (SDM) to generate high-fidelity pulmonary CT images from segmentation maps. We utilize annotation information from the LUNA16 dataset to create paired CT images and masks, and assess the quality of the generated images using the Frechet Inception Distance (FID), as well as on two common clinical downstream tasks: nodule detection and nodule localization. Achieving improvements of 3.96% for detection accuracy and 8.50% for AP50 in nodule localization task, respectively, demonstrates the feasibility of the approach.","link":"http://arxiv.org/abs/2305.01138v1","created":"2023-05-02","tags":["arxiv","data-quality"],"meta":{"query":"data AND quality"}}
